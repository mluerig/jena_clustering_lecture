{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9bd0fc-192b-4507-8994-ecbde503e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dependencies\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "os.chdir(r\"D:\\git-repos\\mluerig\\jena_clustering_lecture\")\n",
    "from utils import extract_features, resize_with_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d079c-33a6-4479-9016-4886077d32e4",
   "metadata": {},
   "source": [
    "# Computer vision\n",
    "\n",
    "In this notebook we will prepare a small dataset by extracting individual leaves from batch scans and thereby explore the basics of computer vision (CV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1d427-4152-4f58-ae41-d46a275c7adb",
   "metadata": {},
   "source": [
    "## Thresholding images \n",
    "\n",
    "Signal processing is a classic field of , the automatic extraction of meaningful information from images. Thresholding is a simple yet effective type of image segmentation technique used in signal processing. It involves converting a grayscale image into a binary image, where the pixels in the image are set to black or white based on a threshold value. \n",
    "\n",
    "This method is particularly useful in separating foreground from the background, simplifying the image analysis. In the context of biological imaging, thresholding can be crucial for tasks such as isolating regions of interest, like individual leaves in scans of plants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cae74-1cf7-4dad-923d-67254db95cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load image\n",
    "image_path = r'data\\eucalyptus_example.png'\n",
    "img = cv2.imread(image_path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3048d-572d-4752-b765-e8df777e2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to grayscale\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef58411-7e0e-4ab2-b908-1fe448d5dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blurring + threshold\n",
    "blurred = cv2.GaussianBlur(gray,(5,5),0)\n",
    "ret, thresh = cv2.threshold(blurred,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "plt.imshow(thresh, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65e638-6d7a-41ab-b9e6-f8ffac110da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## integer encoding (creating a label map)\n",
    "ret, markers = cv2.connectedComponents(thresh)\n",
    "plt.imshow(markers)\n",
    "print(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18670f46-9a2a-4208-9ae5-3c16ba6d3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select a single object\n",
    "mask = np.zeros(markers.shape, dtype=np.uint8)\n",
    "mask[markers == 6]=255\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544002b8-6b83-4cce-80cb-d1ed64272fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "x, y, w, h = cv2.boundingRect(contours[0])\n",
    "plt.imshow(gray[y:y+h, x:x+w], cmap=\"gray\")\n",
    "print(x,y,w,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8412a8-cf82-4fea-9974-ee2fc1e404d9",
   "metadata": {},
   "source": [
    "## Shape and texture traits (features)\n",
    "\n",
    "**Shape traits** are calculated using the contour coordinates of segmented images - e.g., area, perimeter length, and circularity. Spatial moments include information about on object's centroid, orientation, and scale. There are three types of spatial moments:\n",
    "\n",
    "- Spatial (Raw) Moments: These are directly calculated from the object's coordinates, describing its shape\n",
    "- Central Moments: Deviations from the mean, variance and skewness of the centroid.\n",
    "- Normalized Moments (Also known as Hu Moments): derivatives of central moments - scale, translation and rotation invariant.\n",
    "\n",
    "**Texture traits**, derived from analyzing all pixels within the contours, encompass features like color distribution, contrast, and specific visual patterns, facilitating detailed assessments of species variation and environmental adaptation. Texture moments resemble the statistical moments of a distribution: mean intensities of all pixels, as well as their variance, skewness and kurtosis. Additionally, there are some types of higher level features, such as GLCM and GLDM:\n",
    "\n",
    "- GLCM (Gray Level Co-occurrence Matrix): A statistical method that analyzes texture by calculating the frequency of co-occurring pixel values at a specified spatial relationship within an image to derive metrics like contrast and homogeneity.\n",
    "- GLDM (Gray Level Difference Matrix): A technique that evaluates texture by examining the probability distributions of gray level differences between pairs of pixels, allowing the calculation of texture descriptors such as contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2975178-4ea5-4605-90f6-6e4b13969c24",
   "metadata": {},
   "source": [
    "## A note on invariances\n",
    "\n",
    "It is important to be aware of the fact that some features can change depending on an image's orientation, or the location and rotation of the object it contains. So chose the features you would like to extract carefully!\n",
    "\n",
    "- Translation Invariance: Maintains consistency when objects move within an image. Achieved by normalizing to a central point for shape or analyzing location-independent patterns for texture.\n",
    "- Rotation Invariance: Ensures consistent feature identification regardless of object orientation, using methods like circularity for shape or rotation-invariant filters for texture.\n",
    "- Scaling Invariance: Features remain consistent despite object size changes, through normalization of feature vectors or scale-independent abstraction techniques.\n",
    "- Illumination Invariance: Addresses lighting variations by adjusting image lighting conditions or using features less sensitive to illumination changes, such as edge orientation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55725480-7487-4552-9431-686f3cc48db9",
   "metadata": {},
   "source": [
    "## Let's extract some traits! \n",
    "\n",
    "We'll extract some basic features from our leaves: area, maximum diameter, and circularity for shape, and the first for moments of distribution of pixel values (mean, variance, skewness, kurtosis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572dacd-47ac-4e0e-b506-247d88b41bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## contour == perimeter coordinates\n",
    "contour = contours[0]\n",
    "\n",
    "area = cv2.contourArea(contour)\n",
    "diameter = int(cv2.minEnclosingCircle(contour)[1] * 2)\n",
    "circularity = (4 * np.pi * area) / (cv2.arcLength(contour, True) ** 2)\n",
    "\n",
    "print(area, diameter, circularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a735f8a-3ba3-49d9-8616-4e42e3e3fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROI == pixels inside contour (after applying mask)\n",
    "roi = gray[y:y+h, x:x+w]\n",
    "roi_mask = np.logical_not(mask[y:y+h, x:x+w])\n",
    "masked_array = np.ma.masked_array(roi, mask=roi_mask)\n",
    "\n",
    "mean = np.ma.mean(masked_array)\n",
    "variance = np.ma.var(masked_array)\n",
    "skewness = stats.skew(masked_array.compressed())\n",
    "kurtosis = stats.kurtosis(masked_array.compressed()) \n",
    "\n",
    "print(mean, variance, skewness, kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a7ca3-9299-4a34-921d-ed8e5874d9b8",
   "metadata": {},
   "source": [
    "## Final pipeline\n",
    "\n",
    "Wrapping it all up in a single pipeline - segmentation and trait extraction, it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d708d39-01de-483b-94ea-18e6efbd0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare variables\n",
    "count = 0 \n",
    "results = dict()\n",
    "image_name = os.path.basename(image_path)\n",
    "roi_dir = r\"data/rois\"\n",
    "os.makedirs(roi_dir, exist_ok=True)\n",
    "\n",
    "# iterate over the instances and isolate the leavess\n",
    "for i in range(1,markers.max()+1):\n",
    "    \n",
    "    # create a mask for the current marker\n",
    "    mask = np.zeros(markers.shape, dtype=np.uint8)\n",
    "    mask[markers == i] = 255\n",
    "\n",
    "    # find the contours of the mask\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "\n",
    "    if w < 150 and h > 100:\n",
    "\n",
    "        ## advance counter\n",
    "        count += 1\n",
    "        print(count)\n",
    "\n",
    "        ## get contour, ROI, and ROI-mask\n",
    "        contour = contours[0]\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "        roi_mask = mask[y:y+h, x:x+w]\n",
    "\n",
    "        ## extract features and save to dictionary\n",
    "        results[count] = extract_features(contour, gray, mask)\n",
    "\n",
    "        ## save a resized and padded version of the ROI\n",
    "        image = resize_with_pad(roi, (512, 512))\n",
    "        cv2.imwrite(os.path.join(roi_dir, f'{image_name}_{count}.jpg'), image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079e316-4c57-4740-a72a-d74df7ad48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "df.columns = ['Area', 'Diameter', 'Circularity', 'Mean', 'Variance', 'Skewness', 'Kurtosis']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e998b88-de26-423c-bf66-bfaa6af14a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
