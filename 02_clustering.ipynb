{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets import make_moons\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image \n",
    "\n",
    "os.chdir(r\"D:\\git-repos\\mluerig\\jena_clustering_lecture\")\n",
    "from utils import refactor_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this notebook we will use clustering, a type of unsupervised machine learning, to sort leaves into distinct groups based on their traits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the k-means algorithm\n",
    "\n",
    "K-means clustering  is a method that partitions data into 'k' distinct clusters. The 'means' in k-means refers to the centroids or the geometric centers of these clusters. The algorithm iteratively assigns each data point to the nearest cluster, while optimizing the positions of the cluster centroids. \n",
    "\n",
    "This algorithm's beauty lies in its simplicity and effectiveness, making it a widely used tool in various fields, including biology. In biological contexts, k-means can be instrumental in grouping similar gene expressions, categorizing types of cells in imaging data, or even in ecological niche partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working principles\n",
    "\n",
    "The *k*-means algorithm operates on the principle of partitioning a dataset into a specified number of clusters, each defined by its unique characteristics. This algorithm executes its task based on two key concepts:\n",
    "\n",
    " - The center of a cluster, referred to as the 'cluster centroid', is computed as the average of all data points assigned to that cluster.\n",
    " - Every data point is associated with the nearest cluster centroid, ensuring minimal distance from each point to its corresponding center.\n",
    "\n",
    "These principles form the core of the *k*-means model. We will explore the specific mechanics of how the algorithm achieves this clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herbarium leaf trait dataset: 4 species of eucalyptus\n",
    "\n",
    "In line with the unsupervised nature of this algorithm, our initial exploration will be conducted without utilizing any labels for the data points. This approach underscores the algorithm's ability to identify and group data based purely on the inherent structures within the dataset itself.\n",
    "\n",
    "We first load the dataset as a dataframe, and then convert it to a numpy array so it's a bit easier to work with. I have pre-selected two traits: circularity (shape) and skewness (texture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the directory\n",
    "image_files = [f for f in os.listdir(r\"data/examples\") if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(1, len(image_files), figsize=(20, 5))  # Adjust figure size as needed\n",
    "\n",
    "# Loop over the files and plot each image\n",
    "for ax, img_file in zip(axes, image_files):\n",
    "    img_path = os.path.join(r\"data/examples\", img_file)\n",
    "    image = Image.open(img_path)\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')  # Turn off axis labels\n",
    "    ax.set_title(img_file)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading trait dataset and plotting two traits\n",
    "data_euc = pd.read_csv(r\"data\\eucalyptus_traits.csv\")\n",
    "plt.scatter(data_euc[\"circularity\"], data_euc[\"gray_firstorder_Skewness\"], s=25)\n",
    "plt.xlabel('Circularity')\n",
    "plt.ylabel('Skewness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have four well define clusters.\n",
    "Let's see how the *k*-means algorithm groups those data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clustering\n",
    "X = data_euc[[\"circularity\", \"gray_firstorder_Skewness\"]].to_numpy()\n",
    "kmeans = KMeans(n_clusters=4, n_init=100)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the cluster assignments as well as the centroids of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "plt.xlabel('Circularity')\n",
    "plt.ylabel('Skewness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with the true species clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))  # figsize controls the total size of the figure\n",
    "\n",
    "# First plot\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=refactor_array(y_kmeans), s=50, cmap='viridis')\n",
    "ax1.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "ax1.set_title('Prediction')\n",
    "ax2.set_xlabel('Circularity')\n",
    "ax2.set_ylabel('Skewness')\n",
    "\n",
    "# Second plot\n",
    "plt.scatter(data_euc['circularity'], data_euc[\"gray_firstorder_Skewness\"], \n",
    "            c=pd.factorize(data_euc[\"species\"])[0], cmap='viridis')\n",
    "ax2.set_title('Truth')\n",
    "ax2.set_xlabel('Circularity')\n",
    "ax2.set_ylabel('Skewness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encouraging aspect of the *k*-means algorithm is its ability to categorize points into clusters in a manner quite similar to human intuition. This naturally raises a question: how does the algorithm efficiently identify these clusters, especially considering the astronomically high number of possible groupings in datasets with numerous data points? An exhaustive search through all possible combinations would be impractical due to its immense computational demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means Algorithm: Expectation–Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *k*-means algorithm doesn't actually conduct an exhaustive search, but instead employs a more pragmatic and iterative method known as *expectation–maximization*. This approach smartly navigates through the data, making the task of finding clusters far more computationally manageable and efficient than one might initially assume, which makes Expectation-maximization (E-M) a crucial algorithm in data science\n",
    "\n",
    "It involves the following steps:\n",
    "\n",
    "1. __Initial Setup - Choosing Cluster Centers__:\n",
    "\n",
    " - The process begins by selecting initial positions for the cluster centers. This can be done randomly or based on specific criteria.\n",
    "\n",
    "2. __Iterative Process__:\n",
    "\n",
    " - The algorithm then iterates through two main steps:\n",
    "\n",
    "   __A. E-Step (Expectation)__:\n",
    "\n",
    "    - In this step, each data point is assigned to the nearest cluster center. This assignment is based on the distance between the data point and the cluster centers.\n",
    "   \n",
    "   __B. M-Step (Maximization)__:\n",
    "\n",
    "    - Here, the algorithm recalculates the position of each cluster center. The new position is determined by computing the mean of all data points assigned to that cluster.\n",
    "3. __Convergence__:\n",
    "\n",
    " - These two steps (E-step and M-step) are repeated until the positions of the cluster centers stabilize and no longer change significantly. This indicates that the clusters have been effectively identified.\n",
    "4. __Result and Analysis__:\n",
    "\n",
    " - Once the algorithm has converged, the final cluster centers define the grouping of the data. These clusters can then be analyzed to understand the underlying patterns and structures in the data.\n",
    "The E-M algorithm in k-means is efficient and effective for many practical applications, particularly in organizing and understanding large datasets.\n",
    "\n",
    "In summary: with each cycle of the E-step and M-step, the estimation of the cluster attributes improves. This is a key strength of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's see the expectation-maximation algorithm in action. Let's create a simple implementation of the *k*-means algorithm and then plot cluster assignments at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def store_cluster_data(X, centers, labels, n_clusters, plots_data):\n",
    "    plots_data.append((X.copy(), centers.copy(), labels.copy()))\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=42):\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "\n",
    "    plots_data = []\n",
    "    while True:\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        new_centers = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n",
    "\n",
    "        store_cluster_data(X, centers, labels, n_clusters, plots_data)\n",
    "\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "\n",
    "    # Determine the number of rows and columns for the subplots\n",
    "    num_iterations = len(plots_data)\n",
    "    max_cols = 5\n",
    "    num_rows = num_iterations // max_cols + (num_iterations % max_cols > 0)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(num_rows, max_cols, figsize=(5 * max_cols, 5 * num_rows))\n",
    "    axs = axs.flatten()  # Flatten in case of a single row\n",
    "\n",
    "    # Plot each iteration\n",
    "    for idx, (X_iter, centers_iter, labels_iter) in enumerate(plots_data):\n",
    "        axs[idx].scatter(X_iter[:, 0], X_iter[:, 1], c=labels_iter, s=50, cmap='viridis')\n",
    "        axs[idx].scatter(centers_iter[:, 0], centers_iter[:, 1], c='red', s=200, alpha=0.5)\n",
    "        axs[idx].set_title(f\"Iteration {idx+1}\")\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for ax in axs[num_iterations:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return centers, labels\n",
    "\n",
    "# Example usage with your data 'X'\n",
    "centers, labels = find_clusters(X, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats - Local vs Global Minima\n",
    "\n",
    "Like many iterative algorithms, there are a few issues to be aware of when using the *k*-means algorithm. One of these is the problem of converging to a local minimum.\n",
    "Although the E-M procedure will navigate the loss landscape and improve the result in each step, there is no assurance that it will lead to the *global* minima. Certain seeds can lead to suboptimal clustering results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "centers, labels = find_clusters(X, 4, rseed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a common issue with many iterative algorithms, and the solution is to run the algorithm multiple times with different initializations (``n_init``). For that reason, Scikit-Learn does, by default, set by ``n_init`` parameter to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-selecting the Number of Clusters\n",
    "A notable limitation of the *k*-means algorithm is its requirement for pre-specifying the number of clusters. Unlike some other algorithms, *k*-means doesn't have the capability to determine the optimal number of clusters directly from the data. \n",
    "For instance, if we instruct the algorithm to form eight clusters, it will follow our direction and identify the best possible configuration of exactly eight clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## eight clusters\n",
    "labels = KMeans(8, random_state=0, n_init=15).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution to this problem is known as the *Elbow Method*. It involves calculating the Within-Cluster Sum of Squares (WCSS) for a range of cluster numbers and then plotting these values. The \"elbow\" of the plot, where the rate of decrease sharply changes, can be a good indicator of the appropriate number of clusters.\n",
    "\n",
    "Here's a step-by-step guide along with the code:\n",
    "\n",
    "1. Calculate WCSS for Different Number of Clusters:\n",
    "\n",
    " - Iterate over a range of cluster numbers (e.g., 1 to 10).\n",
    " - For each number, fit the KMeans model and calculate the WCSS.\n",
    " \n",
    "2. Plot the Results:\n",
    "\n",
    " - Plot the number of clusters against the WCSS.\n",
    " - Look for the \"elbow\" where the decrease in WCSS becomes less pronounced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of possible clusters\n",
    "n_clusters = range(1, 11)\n",
    "\n",
    "# Empty list to store WCSS\n",
    "wcss = []\n",
    "\n",
    "# Calculate WCSS for each number of clusters\n",
    "for n in n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0, n_init=15).fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_clusters, wcss, 'bo-')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the result is meaningful is a question that is difficult to answer definitively; one approach that is rather intuitive, but that we won't discuss further here, is called [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).\n",
    "\n",
    "Alternatively, you might use a more complicated clustering algorithm which has a better quantitative measure of the fitness per number of clusters (e.g., Gaussian mixture models; see [In Depth: Gaussian Mixture Models](05.12-Gaussian-Mixtures.ipynb)) or which *can* choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or affinity propagation, all available in the ``sklearn.cluster`` submodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Nature of Cluster Boundaries in k-means\n",
    "The intrinsic assumptions of *k*-means, primarily that each point is closer to the centroid of its own cluster than to any other, inherently shape the algorithm's effectiveness. This assumption leads to a significant limitation: k-means tends to create clusters with linear boundaries. As a result, it struggles with complex cluster geometries where linear separations do not suffice.\n",
    "\n",
    "This limitation becomes particularly evident in scenarios where clusters have non-linear, intricate boundaries. Such situations reveal the inadequacy of *k*-means in capturing the true essence of the data's underlying structure. The following example demonstrates this limitation, showcasing how *k*-means classifies a given dataset with complex geometrical arrangements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(200, noise=.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "labels = KMeans(2, random_state=0, n_init=15).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of many solutions : Spectral Clustering\n",
    "\n",
    "A kernelized version of *k*-means is available in Scikit-Learn, implemented through the `SpectralClustering` estimator. This method leverages the concept of nearest neighbor graphs to transform the data into a higher-dimensional space. Once in this expanded space, the algorithm applies a traditional *k*-means approach to assign labels to the data points.\n",
    "\n",
    "This process allows for more complex cluster geometries than standard *k*-means, as it essentially captures the data's manifold structure before applying the clustering algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "labels = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets._samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
